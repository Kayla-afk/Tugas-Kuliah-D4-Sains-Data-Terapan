{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOdqGACiStilG0Nrz+5eRcA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MCYybLh2Rn-A"},"outputs":[],"source":["# Step1: Import the required Python libraries\n","import numpy as np\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","source":["# Step2: Define Activation Function : Sigmoid Function\n","def sigmoid(Z):\n","    return 1 / (1 + np.exp(-Z))"],"metadata":{"id":"4h-OE-yCRxlQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step3: Initialize neural network parameters (weights, bias)\n","# Initialized all the weights in the range between 0 and 1\n","# Bias values are initialized to 0\n","def initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures):\n","    W1 = np.random.randn(neuronsInHiddenLayers, inputFeatures)\n","    W2 = np.random.randn(outputFeatures, neuronsInHiddenLayers)\n","    b1 = np.zeros((neuronsInHiddenLayers, 1))\n","    b2 = np.zeros((outputFeatures, 1))\n","\n","    parameters = {\"W1\": W1, \"b1\": b1,\n","                  \"W2\": W2, \"b2\": b2}\n","    return parameters"],"metadata":{"id":"fzcDVyBrRzQf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step4: Forward Propagation\n","def forwardPropagation(X, Y, parameters):\n","    m = X.shape[1]\n","    W1 = parameters[\"W1\"]\n","    W2 = parameters[\"W2\"]\n","    b1 = parameters[\"b1\"]\n","    b2 = parameters[\"b2\"]\n","    Z1 = np.dot(W1, X) + b1\n","    A1 = sigmoid(Z1)\n","    Z2 = np.dot(W2, A1) + b2\n","    A2 = sigmoid(Z2)\n","\n","    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2)\n","    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))\n","    cost = -np.sum(logprobs) / m\n","    return cost, cache, A2"],"metadata":{"id":"ACykZxWLR1TH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step5: Backward Propagation\n","def backwardPropagation(X, Y, cache):\n","    m = X.shape[1]\n","    (Z1, A1, W1, b1, Z2, A2, W2, b2) = cache\n","\n","    dZ2 = A2 - Y\n","    dW2 = np.dot(dZ2, A1.T) / m\n","    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n","\n","    dA1 = np.dot(W2.T, dZ2)\n","    dZ1 = np.multiply(dA1, A1 * (1 - A1))\n","    dW1 = np.dot(dZ1, X.T) / m\n","    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n","\n","    gradients = {\"dW1\": dW1, \"db1\": db1,\n","                 \"dW2\": dW2, \"db2\": db2}\n","    return gradients"],"metadata":{"id":"Iza7qnOnR37N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step6: Update weight and bias parameters\n","# Updating the weights based on the negative gradients\n","def updateParameters(parameters, gradients, learningRate):\n","    parameters[\"W1\"] = parameters[\"W1\"] - learningRate * gradients[\"dW1\"]\n","    parameters[\"W2\"] = parameters[\"W2\"] - learningRate * gradients[\"dW2\"]\n","    parameters[\"b1\"] = parameters[\"b1\"] - learningRate * gradients[\"db1\"]\n","    parameters[\"b2\"] = parameters[\"b2\"] - learningRate * gradients[\"db2\"]\n","    return parameters"],"metadata":{"id":"0K9M3S4GR51u","executionInfo":{"status":"ok","timestamp":1747218846049,"user_tz":-420,"elapsed":30,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# # Step7: Train the learning model\n","# # Model to learn the AND truth table\n","# X = np.array([[0, 0, 1, 1],   # AND input 1\n","#               [0, 1, 0, 1]])  # AND input 2\n","# Y = np.array([[0, 0, 0, 1]])  # AND output\n","\n","# Step7: Train the learning model (untuk logika OR)\n","X = np.array([[0, 0, 1, 1],   # input 1\n","              [0, 1, 0, 1]])  # input 2\n","Y = np.array([[0, 1, 1, 1]])  # output OR\n","\n","\n","# Define model parameters\n","neuronsInHiddenLayers = 2  # number of hidden layer neurons (2)\n","inputFeatures = X.shape[0]  # number of input features (2)\n","outputFeatures = Y.shape[0]  # number of output features (1)\n","parameters = initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures)\n","epoch = 100000\n","learningRate = 0.01\n","losses = np.zeros((epoch, 1))\n","\n","for i in range(epoch):\n","    losses[i, 0], cache, A2 = forwardPropagation(X, Y, parameters)\n","    gradients = backwardPropagation(X, Y, cache)\n","    parameters = updateParameters(parameters, gradients, learningRate)"],"metadata":{"id":"Qw6vmfvgSMES"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step8: Plot Loss value vs Epoch\n","# Evaluating the performance\n","plt.figure()\n","plt.plot(losses)\n","plt.xlabel(\"EPOCHS\")\n","plt.ylabel(\"Loss value\")\n","plt.show()"],"metadata":{"id":"mrZuPUqhSOU8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step9: Test the model performance\n","# Testing\n","X = np.array([[1, 1, 0, 0], [0, 1, 0, 1]])  # AND input\n","cost, _, A2 = forwardPropagation(X, Y, parameters)\n","prediction = (A2 > 0.5) * 1.0\n","print(prediction)"],"metadata":{"id":"M1CRdVxSSQZ8"},"execution_count":null,"outputs":[]}]}
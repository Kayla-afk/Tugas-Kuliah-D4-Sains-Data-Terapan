{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPslGRoyM4eDR2pQrixnK5Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Converting to lower case**"],"metadata":{"id":"T83VQTq0W-NP"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hf79n4zZWPiM","executionInfo":{"status":"ok","timestamp":1756295844348,"user_tz":-420,"elapsed":57,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"c18a8106-c624-43cb-e1fd-48204e5816da"},"outputs":[{"output_type":"stream","name":"stdout","text":["ini adalah sebuah contoh kalimat untuk konversi huruf kecil\n"]}],"source":["def lower_case_convertion(text):\n","  \"\"\"\n","  Input : - string\n","  Output : - lowercase string\n","  \"\"\"\n","  lower_text = text.lower()\n","  return lower_text\n","\n","ex_lowercase = \"Ini Adalah sebuah contoh kalimat untuk KONVERSI huruf kecil\"\n","lowercase_result = lower_case_convertion(ex_lowercase)\n","print(lowercase_result)"]},{"cell_type":"markdown","source":["**Removal of HTML tags**\n","HTML tags removal implementation using regex module"],"metadata":{"id":"vPrPDFlQW3qL"}},{"cell_type":"code","source":["import re\n","def remove_html_tags(text):\n","  \"\"\"\n","  Return : - String without html tags\n","  input : - String\n","  Output : - String\n","  \"\"\"\n","  html_pattern = r'<.*?>'\n","  without_html = re.sub(pattern=html_pattern, repl='', string=text)\n","  return without_html\n","\n","ex_htmltags = \"\"\" <body>\n","<div>\n","<h1>Halo, ini adalah contoh sebuah teks dengan tag Html. </h1>\n","</div>\n","</body>\n","\"\"\"\n","htmltags_result = remove_html_tags(ex_htmltags)\n","print(f\"Result : - \\n {htmltags_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YKiUJ19iW8L6","executionInfo":{"status":"ok","timestamp":1756296257319,"user_tz":-420,"elapsed":18,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"0e9cf9fc-62c6-4979-b4b4-7756fe8aaa2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Result : - \n","  \n","\n","Halo, ini adalah contoh sebuah teks dengan tag Html. \n","\n","\n","\n"]}]},{"cell_type":"markdown","source":["Implementation of Removing HTML tags using bs4 library"],"metadata":{"id":"2I_bS9oAamXs"}},{"cell_type":"code","source":["from bs4 import BeautifulSoup\n","def remove_html_tags_beautifulsoup(text):\n","  \"\"\"\n","  Return : - String without Html tags\n","  Input : - String\n","  Output : - String\n","  \"\"\"\n","  parser = BeautifulSoup(text, \"html.parser\")\n","  without_html = parser.get_text(separator =\" \")\n","  return without_html\n","\n","ex_htmltags = \"\"\" <body>\n","<div>\n","<h1>Halo, ini adalah contoh sebuah teks dengan tag Html. </h1>\n","</div>\n","</body>\n","\"\"\"\n","htmltags_result = remove_html_tags_beautifulsoup(ex_htmltags)\n","print(f\"Result : - \\n {htmltags_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCNOg3Gwamu3","executionInfo":{"status":"ok","timestamp":1756296624900,"user_tz":-420,"elapsed":63,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"fea18579-2243-47a7-f8e4-6ee8bed3d3df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Result : - \n","   \n"," \n"," Halo, ini adalah contoh sebuah teks dengan tag Html.  \n"," \n"," \n","\n"]}]},{"cell_type":"markdown","source":["**Removal of URLs**"],"metadata":{"id":"hhvTEfd1b91g"}},{"cell_type":"code","source":["import re\n","def remove_urls(text):\n","  \"\"\"\n","  Return : - String without URLs\n","  input : - String\n","  Output : - String\n","  \"\"\"\n","  url_pattern = r'https?://S+|www\\.\\S+'\n","  without_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n","  return without_urls\n","\n","ex_urls = \"\"\"\n","Ini adalah contoh teks untuk URL seperti http://google.com & https://www.facebook.com/ etc.\n","\"\"\"\n","\n","urls_result = remove_urls(ex_urls)\n","print(f\"Result after removing URLs from text :- \\n {urls_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"upT5MrWWcBjz","executionInfo":{"status":"ok","timestamp":1756297092367,"user_tz":-420,"elapsed":20,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"1bf8adde-a4e9-4b55-d7ea-40fbd927bd68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Result after removing URLs from text :- \n"," \n","Ini adalah contoh teks untuk URL seperti http://google.com & https://  etc.\n","\n"]}]},{"cell_type":"markdown","source":["**Removing Numbers**"],"metadata":{"id":"-CDCSZ8OdxlJ"}},{"cell_type":"code","source":["import re\n","\n","def remove_numbers(text):\n","    \"\"\"\n","    Return : - String without numbers\n","    Input  : - String\n","    Output : - String\n","    \"\"\"\n","    number_pattern = r'\\d+'\n","    without_number = re.sub(pattern=number_pattern, repl=' ', string=text)\n","    return without_number\n","\n","ex_numbers = \"\"\"\n","Ini adalah contoh kalimat untuk menghapus nomor seperti 1, 5, 7, 4, 77 etc.\n","\"\"\"\n","\n","numbers_result = remove_numbers(ex_numbers)\n","print(f\"Result after removing numbers from text :- \\n{numbers_result}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nCVbfikXd1s3","executionInfo":{"status":"ok","timestamp":1756462808360,"user_tz":-420,"elapsed":49,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"f78f2135-5293-4242-dffd-7386bd6ec88b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Result after removing numbers from text :- \n","\n","Ini adalah contoh kalimat untuk menghapus nomor seperti  ,  ,  ,  ,   etc.\n","\n"]}]},{"cell_type":"markdown","source":["**Converting numbers to word**"],"metadata":{"id":"ITqBhfN3fnXH"}},{"cell_type":"code","source":["from num2words import num2words\n","\n","def num_to_words(text):\n","    \"\"\"\n","    Returns:\n","        string: Text with all numbers converted to words.\n","    \"\"\"\n","    after_splitting = text.split()\n","\n","    for index in range(len(after_splitting)):\n","        if after_splitting[index].isdigit():\n","            after_splitting[index] = num2words(after_splitting[index])\n","\n","    numbers_to_words = ' '.join(after_splitting)\n","    return numbers_to_words\n","\n","ex_numbers = \"\"\"\n","Ini adalah contoh untuk mengubah angka menjadi kata seperti 1 ke satu, 5 ke lima, 74 ke tujuh puluh empat, dll.\n","\"\"\"\n","\n","numbers_result = num_to_words(ex_numbers)\n","print(f\"Result after converting numbers to words:\\n{numbers_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OOOIP9OsfrBe","executionInfo":{"status":"ok","timestamp":1756462818284,"user_tz":-420,"elapsed":52,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"1d747866-efee-49da-9618-ad887eea2635"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Result after converting numbers to words:\n","Ini adalah contoh untuk mengubah angka menjadi kata seperti one ke satu, five ke lima, seventy-four ke tujuh puluh empat, dll.\n"]}]},{"cell_type":"markdown","source":["**Apply Spelling Correction**\n","Implementation of spelling correction using python pyspellchecker library"],"metadata":{"id":"mWztCXfHiVCU"}},{"cell_type":"code","source":["!pip install pyspellchecker\n","from spellchecker import SpellChecker\n","\n","spell_corrector = SpellChecker()\n","\n","def spell_correction(text):\n","  \"\"\"\n","  Return : - text which have correct spelling words\n","  Input : - string\n","  Output : - string\n","  \"\"\"\n","  correct_words = []\n","  misSpelled_words = spell_corrector.unknown(text.split())\n","\n","  for each_word in text.split():\n","    if each_word in misSpelled_words:\n","      right_word = spell_corrector.correction(each_word)\n","      correct_words.append(right_word)\n","    else:\n","      correct_words.append(each_word)\n","\n","ex_misSpell_words = \"\"\"\n","Ini adalah contoh kalimat untuk mengeja koreksi\n","\"\"\"\n","\n","spell_result = spell_correction(ex_misSpell_words)\n","print(f\"Result after spell checking :- \\n{spell_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"529KFoSnibT3","executionInfo":{"status":"ok","timestamp":1756462964588,"user_tz":-420,"elapsed":8239,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"dc283b45-81a1-423f-efd0-f5764157f4ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.12/dist-packages (0.8.3)\n","Result after spell checking :- \n","None\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d424955a","executionInfo":{"status":"ok","timestamp":1756462766971,"user_tz":-420,"elapsed":12387,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"0fba45b2-f6ca-49ed-d14d-f56c6ac3fe35"},"source":["%pip install num2words"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting num2words\n","  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n","Collecting docopt>=0.6.2 (from num2words)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: docopt\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=a34ca17292cd496bfbea2173cbf2d885baa05981950fc3b83be563b33678c9b9\n","  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n","Successfully built docopt\n","Installing collected packages: docopt, num2words\n","Successfully installed docopt-0.6.2 num2words-0.5.14\n"]}]},{"cell_type":"markdown","source":["Implementation of spelling correction using python autocorrect library"],"metadata":{"id":"wVJPUZe2W1wZ"}},{"cell_type":"code","source":["%pip install autocorrect"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"90Z_pN0cW7gQ","executionInfo":{"status":"ok","timestamp":1756463191712,"user_tz":-420,"elapsed":8853,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"8b0069d1-0e81-47c8-afb2-d57be83fff07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting autocorrect\n","  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: autocorrect\n","  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622364 sha256=b4b40af427178c375d0947535d17d0e882eb84df7d73920512cd249f344c0021\n","  Stored in directory: /root/.cache/pip/wheels/b6/28/c2/9ddf8f57f871b55b6fd0ab99c887531fb9a66e5ff236b82aee\n","Successfully built autocorrect\n","Installing collected packages: autocorrect\n","Successfully installed autocorrect-2.6.1\n"]}]},{"cell_type":"code","source":["from autocorrect import Speller\n","from nltk import word_tokenize\n","\n","def spell_autocorrect(text):\n","  \"\"\"\n","  Return : - text which have correct spelling words\n","  Input : - string\n","  Output : - string\n","  \"\"\"\n","  correct_spell_words = []\n","\n","  spell_corrector = Speller(lang='en')\n","  for word in word_tokenize(text):\n","    correct_word = spell_corrector(word)\n","    correct_spell_words.append(correct_word)\n","\n","  correct_spelling = ''.join(correct_spell_words)\n","  return correct_spelling\n","\n","ex_misSpell_words_1 = \"\"\"\n","Ini adalah contoh kalimat untuk mengeja koreksi\n","\"\"\"\n","spell_result = spell_autocorrect(ex_misSpell_words_1)\n","print(f\"Result :- \\n{spell_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KU1LyxoCXfGG","executionInfo":{"status":"ok","timestamp":1756463658651,"user_tz":-420,"elapsed":405,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"edbfefc2-36d8-4965-c190-b67dd92717e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Result :- \n","Inidalamcontrolklimauntukmengejakoreas\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"41c3676b","executionInfo":{"status":"ok","timestamp":1756463653046,"user_tz":-420,"elapsed":33,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"84b6a9e5-9cbb-4e56-e024-2c3171a706b7"},"source":["import nltk\n","nltk.download('punkt_tab')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["**Convert accented characters to ASCII characters**\n","Implementation of accented text to ASCII converter in python"],"metadata":{"id":"VBwYamerZR66"}},{"cell_type":"code","source":["%pip install unidecode\n","import unidecode\n","\n","def accented_to_ascii(text):\n","  \"\"\"\n","  Return : - text after converting accented characters\n","  Input : - string\n","  Output : - string\n","  \"\"\"\n","  text = unidecode.unidecode(text)\n","  return text\n","\n","ex_accented = \"\"\"\n","Ini adalah contoh teks dengan karakter bereksplisit seperti deep learning dan computer vision dll.\n","\"\"\"\n","accented_result = accented_to_ascii(ex_accented)\n","print(f\"Result after converting accented characters to their ASCII values \\n{accented_result}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ce2wifPoZYi4","executionInfo":{"status":"ok","timestamp":1756464236605,"user_tz":-420,"elapsed":6426,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"ed8a53ac-e9f2-4130-ea9f-2ad12120fe02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: unidecode in /usr/local/lib/python3.12/dist-packages (1.4.0)\n","Result after converting accented characters to their ASCII values \n","\n","Ini adalah contoh teks dengan karakter bereksplisit seperti deep learning dan computer vision dll.\n","\n"]}]},{"cell_type":"markdown","source":["**Converting chat conversion words to normal words**"],"metadata":{"id":"OdiGRLgzba1F"}},{"cell_type":"code","source":["def short_to_original(text):\n","  \"\"\"\n","  Return : - text after converting short_form words to original\n","  Input : - string\n","  Output : - string\n","  \"\"\"\n","  new_text = []\n","  for w in text.split():\n","    if w.upper() in chat_words_list:\n","      new_text.append(chat_words_map_dict[w.upper()])\n","    else:\n","      new_text.append(w)\n","    return \" \".join(new_text)\n","\n","ex_chat = \"\"\"\n","Astaga, ini adalah contoh teks untuk percakapan.\n","\"\"\"\n","short_form_list = open('short_forms.txt', 'r')\n","chat_words_list = []\n","for line in chat_words_str.split(\"\\n\"):\n","  if line != \"\":\n","    cw = line.split(\"=\")[1]\n","    chat_words_list.append(cw)\n","    chat_words_map_dict[cw] = cw_expanded\n","chat_words_list = set(chat_words_list)\n","\n","chat_result = short_to_original(ex_chat)\n","print(f\"Result {chat_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"R2hQw0tsbjSt","executionInfo":{"status":"error","timestamp":1756466830410,"user_tz":-420,"elapsed":31,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"2c857a0e-a558-494f-eb83-6ed398126136"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'short_forms.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1227942370.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mAstaga\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mini\u001b[0m \u001b[0madalah\u001b[0m \u001b[0mcontoh\u001b[0m \u001b[0mteks\u001b[0m \u001b[0muntuk\u001b[0m \u001b[0mpercakapan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \"\"\"\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mshort_form_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'short_forms.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mchat_words_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchat_words_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'short_forms.txt'"]}]},{"cell_type":"code","metadata":{"id":"fbb55cea"},"source":["# Paste the content of your short_forms.txt file here as a multi-line string\n","chat_words_str = \"\"\"\n","# Example format:\n","# BRB=Be Right Back\n","# BTW=By The Way\n","\"\"\"\n","\n","# You can now run the next cell to use this data."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2f4dc2ab"},"source":["# Paste the content of your short_forms.txt file here as a multi-line string\n","chat_words_str = \"\"\"\n","# Contoh format bahasa Indonesia:\n","LOL=Tertawa terbahak-bahak\n","BTW=Ngomong-ngomong\n","\"\"\"\n","\n","# You can now run the next cell to use this data."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fef1e3ee","executionInfo":{"status":"ok","timestamp":1756467012497,"user_tz":-420,"elapsed":76,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"37d9c43e-1a89-492e-c7ce-2f00e6a2f79b"},"source":["chat_words_str = \"\"\"\n","LOL=Tertawa terbahak-bahak\n","BTW=Ngomong-ngomong\n","\"\"\"\n","\n","chat_words_list = []\n","chat_words_map_dict = {} # Initialize the dictionary\n","for line in chat_words_str.split(\"\\n\"):\n","  if line and not line.startswith(\"#\"): # Ignore empty lines and comments\n","    parts = line.split(\"=\")\n","    if len(parts) == 2:\n","      short_form, expanded_form = parts\n","      chat_words_list.append(short_form.strip().upper())\n","      chat_words_map_dict[short_form.strip().upper()] = expanded_form.strip()\n","\n","chat_words_list = set(chat_words_list)\n","\n","def short_to_original(text):\n","  \"\"\"\n","  Return : - text after converting short_form words to original\n","  Input : - string\n","  Output : - string\n","  \"\"\"\n","  new_text = []\n","  for w in text.split():\n","    if w.upper() in chat_words_list:\n","      new_text.append(chat_words_map_dict[w.upper()])\n","    else:\n","      new_text.append(w)\n","  return \" \".join(new_text)\n","\n","ex_chat = \"\"\"\n","Astaga, ini adalah contoh teks untuk percakapan. BTW LOL\n","\"\"\"\n","\n","chat_result = short_to_original(ex_chat)\n","print(f\"Result {chat_result}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Result Astaga, ini adalah contoh teks untuk percakapan. Ngomong-ngomong Tertawa terbahak-bahak\n"]}]},{"cell_type":"markdown","source":["**Expanding Contractions**"],"metadata":{"id":"ngkJNYnlmN3m"}},{"cell_type":"code","source":["%pip install contractions\n","\n","import re\n","import contractions\n","\n","contraction_mapping = contractions.contractions_dict\n","\n","def expand_contractions(contraction):\n","    match = contraction.group(0)\n","    first_char = match[0]\n","    expanded_contraction = contraction_mapping.get(match) or contraction_mapping.get(match.lower())\n","    if expanded_contraction:\n","\n","        expanded_contraction = first_char + expanded_contraction[1:]\n","    else:\n","        expanded_contraction = match\n","    return expanded_contraction\n","\n","contractions_pattern = re.compile('({})'.format('|'.join(re.escape(key) for key in contraction_mapping.keys())),\n","                                  flags=re.IGNORECASE | re.DOTALL)\n","\n","ex_contractions = \"\"\"Terkadang, pikiran kita tidak bisa bekerja secara benar.\"\"\"\n","\n","expanded_text = contractions_pattern.sub(expand_contractions, ex_contractions)\n","expanded_text = re.sub(\" ' \", \"\", expanded_text)\n","\n","print(f\"Result :- \\n{expanded_text}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f9rBvG4PmSUU","executionInfo":{"status":"ok","timestamp":1756468356523,"user_tz":-420,"elapsed":8949,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"92eafba9-7133-473d-ae9d-c6ab0820269e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: contractions in /usr/local/lib/python3.12/dist-packages (0.1.73)\n","Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.12/dist-packages (from contractions) (0.0.24)\n","Requirement already satisfied: anyascii in /usr/local/lib/python3.12/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n","Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.12/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n","Result :- \n","Terkadang, pikiran kita tidak bisa bekerja secara benar.\n"]}]},{"cell_type":"markdown","source":["**Stemming**"],"metadata":{"id":"nLPa5sI4rJGG"}},{"cell_type":"code","source":["%pip install nltk\n","from nltk.stem import PorterStemmer\n","def porter_stemmer(text):\n","  \"\"\"\n","  Result : - string after stemming\n","  Input : - string\n","  Output : - string\n","  \"\"\"\n","  tokens = word_tokenize(text)\n","\n","  for index in range(len(tokens)):\n","    stem_word = stemmer.stem(tokens[index])\n","    tokens[index] = stem_word\n","\n","  return ' '.join(tokens)\n","\n","stemmer = PorterStemmer()\n","ex_stem = \"Seorang programmer membuat program menggunakan bahasa pemrograman\"\n","stem_result = porter_stemmer(ex_stem)\n","print(f\"Result after stemming technique: - \\n{stem_result} \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UKehHm14rMMm","executionInfo":{"status":"ok","timestamp":1756469117932,"user_tz":-420,"elapsed":6829,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"6842fda6-c6f0-492b-9147-935b10e2d7d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n","Result after stemming technique: - \n","seorang programm membuat program menggunakan bahasa pemrograman \n"]}]},{"cell_type":"markdown","source":["**Lemmatization**"],"metadata":{"id":"uhztToZwuFJb"}},{"cell_type":"code","source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","def lemmatization(text):\n","    \"\"\"\n","    Result : - string after lemmatization\n","    Input  : - string\n","    Output : - string\n","    \"\"\"\n","    tokens = word_tokenize(text)\n","    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","    return ' '.join(lemmatized_tokens)\n","\n","ex_lemma = \"\"\"Para programmer membuat program menggunakan bahasa pemrograman\"\"\"\n","\n","lemma_result = lemmatization(ex_lemma)\n","print(f\"Result of lemmatization \\n{lemma_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9OViAeNuKc2","executionInfo":{"status":"ok","timestamp":1756469670608,"user_tz":-420,"elapsed":5143,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"5a4fbfde-3524-4d94-9890-143cf0d4aa67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Result of lemmatization \n","Para programmer membuat program menggunakan bahasa pemrograman\n"]}]},{"cell_type":"markdown","source":["**Removal of Emojis**"],"metadata":{"id":"hrIqU-NnwIAt"}},{"cell_type":"code","source":["import re\n","\n","def remove_emojis(text):\n","    \"\"\"\n","    Result :- string without any emojis in it\n","    Input  :- String\n","    Output :- String\n","    \"\"\"\n","    emoji_pattern = re.compile(\n","        \"[\"\n","        u\"\\U0001F600-\\U0001F64F\"\n","        u\"\\U0001F300-\\U0001F5FF\"\n","        u\"\\U0001F680-\\U0001F6FF\"\n","        u\"\\U0001F1E0-\\U0001F1FF\"\n","        u\"\\U00002500-\\U00002BEF\"\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        u\"\\U0001f926-\\U0001f937\"\n","        u\"\\U00010000-\\U0010ffff\"\n","        u\"\\u2640-\\u2642\"\n","        u\"\\u2600-\\u2B55\"\n","        u\"\\u200d\"\n","        u\"\\u23cf\"\n","        u\"\\u23e9\"\n","        u\"\\u231a\"\n","        u\"\\ufe0f\"\n","        u\"\\u3030\"\n","        \"]+\", flags=re.UNICODE\n","    )\n","    return emoji_pattern.sub(r'', text)\n","\n","ex_emoji = \"\"\"Ini adalah sebuah contoh üòäüöÄüåç. Emoji dan emotikon berbeda! ¬Ø\\\\_(„ÉÑ)_/¬Ø\"\"\"\n","\n","emoji_result = remove_emojis(ex_emoji)\n","print(f\"Result text after removing emojis :- \\n{emoji_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YFF5wkyVwME4","executionInfo":{"status":"ok","timestamp":1756470174496,"user_tz":-420,"elapsed":11,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"0d523be5-fb1b-48da-c3b8-5ad7ad2345a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Result text after removing emojis :- \n","Ini adalah sebuah contoh . Emoji dan emotikon berbeda! ¬Ø\\_()_/¬Ø\n"]}]},{"cell_type":"markdown","source":["**Emoticons removal**"],"metadata":{"id":"O2OOFXQOyFYo"}},{"cell_type":"code","source":["import re\n","from emoticons_list import EMOTICONS  # Make sure this is a list of emoticon strings\n","\n","def remove_emoticons(text):\n","    \"\"\"\n","    Return :- string after removing emoticons\n","    Input  :- string\n","    Output :- string\n","    \"\"\"\n","    # Safely escape each emoticon for regex\n","    emoticon_pattern = re.compile(u'(' + u'|'.join(re.escape(k) for k in EMOTICONS) + u')')\n","    without_emoticons = emoticon_pattern.sub(r'', text)\n","    return without_emoticons\n","\n","# Example sentence\n","ex_emoticons = \"Hello this is a sentence with these 2 emoticons :-) & :-D\"\n","\n","# Apply the function\n","emoticons_result = remove_emoticons(ex_emoticons)\n","print(f\"After removing emoticons :- \\n{emoticons_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399},"id":"Mnme94ryyST7","executionInfo":{"status":"error","timestamp":1756470559873,"user_tz":-420,"elapsed":83,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"58ef512f-3ec6-49cf-9e9b-5830eed26ca2"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'emoticons_list'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-58767618.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0memoticons_list\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEMOTICONS\u001b[0m  \u001b[0;31m# Make sure this is a list of emoticon strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_emoticons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'emoticons_list'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"id":"43b6c18e"},"source":["EMOTICONS = [\":-)\",\n","             \":-(\",\n","             \":-D\",\n","             \":-P\",\n","             \";-)\",\n","             \":'(\",\n","             \":-|\",\n","             \":-/\",\n","             \":-*\",\n","             \":-O\",\n","             \"XD\",\n","             \"<3\",\n","             \":')\",\n","             \"B-)\",\n","             \"O.o\",\n","             \":3\",\n","             \"xP\",\n","             \":c\",\n","             \":>\",\n","             \":{\",\n","             \":>\",\n","             \"=)\",\n","             \"8)\",\n","             \":|\",\n","             \":\\\\\",\n","             \":*\",\n","             \":&\",\n","             \":$\",\n","            \"=D\",\n","             \"=P\",\n","             \"=(\",\n","             \"=[\",\n","             \":-<>\",\n","             \":-O.\",\n","             \":-]\",\n","             \":^)\",\n","             \":D\",\n","             \":P\",\n","             \":(\",\n","             \":')\",\n","             \":*\",\n","             \";)\",\n","             \":-()\",\n","             \":-o\",\n","             \"8-)\",\n","             \":-/\",\n","             \":-$\",\n","             \":-&\",\n","             \":-@\",\n","             \":-#\",\n","             \":-%\",\n","             \":-!\",\n","             \":-~\",\n","             \":-*\",\n","             \":-`\",\n","             \";(\",\n","             \";)\",\n","             \":-?\",\n","             \":-.\",\n","             \":-\\\\\",\n","             \":-/\",\n","             \":-:\",\n","             \":-!\",\n","             \"xD\",\n","             \"XD\",\n","             \":'D\",\n","             \"8=D\",\n","             \"(:\",\n","             \"):\",\n","             \"XD\",\n","             \"XP\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"15570b93","executionInfo":{"status":"ok","timestamp":1756540645839,"user_tz":-420,"elapsed":12,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"db402514-4b91-4d5b-d0ba-4081e060eb10"},"source":["import re\n","\n","def remove_emoticons(text):\n","    \"\"\"\n","    Return :- string after removing emoticons\n","    Input  :- string\n","    Output :- String\n","    \"\"\"\n","\n","    emoticon_pattern = re.compile(u'(' + u'|'.join(re.escape(k) for k in EMOTICONS) + u')')\n","    without_emoticons = emoticon_pattern.sub(r'', text)\n","    return without_emoticons\n","\n","ex_emoticons = \"Halo, ini adalah kalimat dengan 2 emotikon :-) & :-D\"\n","\n","emoticons_result = remove_emoticons(ex_emoticons)\n","print(f\"After removing emoticons :- \\n{emoticons_result}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["After removing emoticons :- \n","Halo, ini adalah kalimat dengan 2 emotikon  & \n"]}]},{"cell_type":"markdown","source":["**Converting Emojis to Words**"],"metadata":{"id":"CkuXZ2250XAE"}},{"cell_type":"code","source":["import re\n","\n","def emoji_words(text):\n","    \"\"\"\n","    Return :- string after converting emojis to descriptive words\n","    Input  :- string\n","    Output :- string\n","    \"\"\"\n","    for emoji_char, description in EMO_UNICODE.items():\n","        emoji_pattern = re.escape(emoji_char)\n","\n","        emoji_name = description\n","\n","        text = re.sub(emoji_pattern, emoji_name, text)\n","    return text\n","\n","ex_emoji = \"Ini adalah contoh üòª üëçüèø\"\n","\n","emoji_result = emoji_words(ex_emoji)\n","print(f\"Result after converting emojis to corresponding words :- \\n{emoji_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3YvWudRg0fP5","executionInfo":{"status":"ok","timestamp":1756472673187,"user_tz":-420,"elapsed":82,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"b81852e7-27d8-402e-f853-446e2a0ef4a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Result after converting emojis to corresponding words :- \n","Ini adalah contoh smiling_cat_face_with_heart-eyes thumbs_up_dark_skin_tone\n"]}]},{"cell_type":"markdown","source":["**Converting Emoticons to Words**"],"metadata":{"id":"zdb42oge709z"}},{"cell_type":"code","source":["import re\n","\n","def emoticons_words(text):\n","    \"\"\"\n","    Return :- string after converting emoticons to descriptive words\n","    Input  :- string\n","    Output :- string\n","    \"\"\"\n","\n","    EMOTICONS_DICT = {\n","        \":-)\": \"smiling_face\",\n","        \":-D\": \"grinning_face\",\n","        \":-(\": \"sad_face\",\n","        \";-)\": \"winking_face\",\n","    }\n","\n","    for emot, word_representation in EMOTICONS_DICT.items():\n","        emoticon_pattern = re.escape(emot)\n","        text = re.sub(emoticon_pattern, word_representation, text)\n","    return text\n","\n","\n","ex_emoticons = \"Halo, ini adalah 2 emotikon dengan wajah tersenyum :-) & :-)\"\n","\n","emoticons_result = emoticons_words(ex_emoticons)\n","print(f\"After converting emoticons to words :- \\n{emoticons_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uG0m7oPR77sq","executionInfo":{"status":"ok","timestamp":1756473304769,"user_tz":-420,"elapsed":57,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"a8e69bd3-c4d0-476b-c0b7-3f55ca2152ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["After converting emoticons to words :- \n","Halo, ini adalah 2 emotikon dengan wajah tersenyum smiling_face & smiling_face\n"]}]},{"cell_type":"markdown","source":["**Removing of Punctuations or Special Characters**\n","\n","Implementation of removing punctuations using string library"],"metadata":{"id":"ocEkl-Ir99hZ"}},{"cell_type":"code","source":["import string\n","\n","def remove_punctuation(text):\n","    \"\"\"\n","    Return :- String after removing punctuations\n","    Input  :- String\n","    Output :- String\n","    \"\"\"\n","    return text.translate(str.maketrans('', '', string.punctuation))\n","\n","ex_punct = \"Ini adalah contoh teks untuk tanda baca .?/*\"\n","\n","punct_result = remove_punctuation(ex_punct)\n","print(f\"Result after removing punctuations :- \\n{punct_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0M38faN-Jxc","executionInfo":{"status":"ok","timestamp":1756473539823,"user_tz":-420,"elapsed":60,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"73d83828-9d73-4fdf-8e77-afffca83e332"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Result after removing punctuations :- \n","Ini adalah contoh teks untuk tanda baca \n"]}]},{"cell_type":"markdown","source":["**Removing of stopwords**"],"metadata":{"id":"P_-nPtse_CNh"}},{"cell_type":"code","source":["import nltk\n","import spacy\n","import gensim\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import string\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","stopwords_nltk = stopwords.words('english')\n","sp = spacy.load('en_core_web_sm')\n","stopwords_spacy = list(sp.Defaults.stop_words)\n","stopwords_gensim = list(gensim.parsing.preprocessing.STOPWORDS)\n","\n","all_stopwords = list(set(stopwords_nltk + stopwords_spacy + stopwords_gensim))\n","\n","print(f\"Total number of Stopwords :- {len(all_stopwords)}\")\n","\n","def remove_stopwords(text):\n","    \"\"\"\n","    Return :- String after removing stopwords\n","    Input  :- String\n","    Output :- String\n","    \"\"\"\n","    text_tokens = word_tokenize(text)\n","    text_without_sw = [word for word in text_tokens if word.lower() not in all_stopwords]\n","    return ' '.join(text_without_sw)\n","\n","ex_sw = \"this is an example text for stopwords such as a, an, the etc.\"\n","\n","sw_result = remove_stopwords(ex_sw)\n","print(f\"Result after removing stopwords :- \\n{sw_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Ee4zpE5_Hsf","executionInfo":{"status":"ok","timestamp":1756474094529,"user_tz":-420,"elapsed":14629,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"a6e72b53-e36c-45d6-879e-1fcd0706c95e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Total number of Stopwords :- 431\n","Result after removing stopwords :- \n","example text stopwords , , .\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":697},"id":"5be565bb","executionInfo":{"status":"ok","timestamp":1756473857136,"user_tz":-420,"elapsed":26682,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"cc8ed5fc-8a47-4aeb-ec0b-d80d0aef5f92"},"source":["%pip install gensim"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gensim\n","  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Collecting numpy<2.0,>=1.18.5 (from gensim)\n","  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n","  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n","Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.16.1\n","    Uninstalling scipy-1.16.1:\n","      Successfully uninstalled scipy-1.16.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","scipy"]},"id":"d408a08756ec4a2fbcc670b378abbfbd"}},"metadata":{}}]},{"cell_type":"markdown","source":["**Implementation of frequent words removing**"],"metadata":{"id":"1PVVy7ikBCZK"}},{"cell_type":"code","source":["%pip install nltk\n","import nltk\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    nltk.download('punkt')\n","\n","try:\n","    nltk.data.find('tokenizers/punkt_tab')\n","except LookupError:\n","    nltk.download('punkt_tab')\n","\n","\n","def freq_words(text):\n","    \"\"\"\n","    Return the top 10 most frequent alphabetic words from the input text.\n","    \"\"\"\n","    tokens = word_tokenize(text.lower())\n","    words = [word for word in tokens if word.isalpha()]\n","    counter = Counter(words)\n","    return [word for word, count in counter.most_common(10)]\n","\n","def remove_fw(text, FrequentWords):\n","    \"\"\"\n","    Remove the frequent words from the text and return the cleaned version.\n","    \"\"\"\n","    tokens = word_tokenize(text)\n","    filtered = [word for word in tokens if word.lower() not in FrequentWords]\n","    return TreebankWordDetokenizer().detokenize(filtered)\n","\n","ex_fw = \"\"\"Pembelajaran mesin adalah gagasan bahwa ada algoritma generik yang dapat memberi\n","tahu Anda sesuatu yang menarik tentang sebuah data. Misalnya, salah satu jenis\n","algoritma adalah algoritma klasifikasi. Algoritma ini dapat membagi data ke dalam kelompok yang berbeda.\n","Anda dapat berpikir tentang algoritma pembelajaran mesin sebagai jatuh ke dalam\n","salah satu dari dua kategori utama -- pembelajaran terawasi.\n","Katakanlah Anda adalah seorang agen real estat. Bisnis Anda sedang berkembang,\n","jadi Anda mempekerjakan sekelompok agen trainee baru untuk membantu.\n","Untuk membantu trainee Anda (dan mungkin membebaskan diri Anda untuk berlibur),\n","Anda memutuskan untuk menulis aplikasi kecil yang dapat memperkirakan harga.\n","Ini disebut pembelajaran terawasi. Anda tahu berapa banyak setiap rumah terjual,\n","jadi dengan kata lain, Anda tahu jawabannya. Untuk membangun aplikasi Anda,\n","Anda memberikan data pelatihan tentang setiap rumah ke dalam algoritma pembelajaran mesin Anda.\"\"\"\n","\n","FrequentWords = freq_words(ex_fw)\n","print(\"Top 10 Frequent Words:\\n\", FrequentWords)\n","\n","fw_result = remove_fw(ex_fw, FrequentWords)\n","print(\"\\nText after removing frequent words:\\n\", fw_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YvwOLodRBIvr","executionInfo":{"status":"ok","timestamp":1756548041905,"user_tz":-420,"elapsed":6935,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"8b80f59f-16af-4484-8943-8cf159dae43f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Top 10 Frequent Words:\n"," ['anda', 'algoritma', 'pembelajaran', 'untuk', 'yang', 'dapat', 'mesin', 'adalah', 'tahu', 'tentang']\n","\n","Text after removing frequent words:\n"," gagasan bahwa ada generik memberi sesuatu menarik sebuah data . Misalnya, salah satu jenis klasifikasi . ini membagi data ke dalam kelompok berbeda . berpikir sebagai jatuh ke dalam salah satu dari dua kategori utama--terawasi . Katakanlah seorang agen real estat . Bisnis sedang berkembang, jadi mempekerjakan sekelompok agen trainee baru membantu . membantu trainee (dan mungkin membebaskan diri berlibur), memutuskan menulis aplikasi kecil memperkirakan harga . Ini disebut terawasi . berapa banyak setiap rumah terjual, jadi dengan kata lain, jawabannya . membangun aplikasi, memberikan data pelatihan setiap rumah ke dalam.\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]}]},{"cell_type":"markdown","source":["**Removing of rare words**"],"metadata":{"id":"pQAyWZD5DXE6"}},{"cell_type":"code","source":["import nltk\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","def rare_words(text, number_rare_words=10):\n","    \"\"\"\n","    Return :- Most Rare words\n","    Input  :- string\n","    Output :- list of rare words\n","    \"\"\"\n","    tokens = word_tokenize(text.lower())\n","    counter = Counter(tokens)\n","\n","    rare = counter.most_common()[:-number_rare_words-1:-1]\n","    RareWords = [word for word, count in rare]\n","    return RareWords\n","\n","def remove_rw(text, RareWords):\n","    \"\"\"\n","    Return :- String after removing rare words\n","    Input  :- String\n","    Output :- String\n","    \"\"\"\n","    tokens = word_tokenize(text)\n","    without_rw = [word for word in tokens if word.lower() not in RareWords]\n","    return ' '.join(without_rw)\n","\n","ex_fw = \"\"\"\n","Untuk membangun aplikasi Anda, Anda memberi data pelatihan tentang setiap rumah\n","ke dalam algoritma pembelajaran mesin Anda. Ini seperti memiliki kunci jawaban\n","untuk ujian matematika dengan semua simbol aritmatika yang dihapus.\n","\"\"\"\n","\n","RareWords = rare_words(ex_fw)\n","print(f\"Top 10 Rarer Words from our example text :- \\n{RareWords}\\n\")\n","\n","rw_result = remove_rw(ex_fw, RareWords)\n","print(f\"Result after removing rare words :-\\n{rw_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zEqIgMzzFFw9","executionInfo":{"status":"ok","timestamp":1756548293831,"user_tz":-420,"elapsed":44,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"fc7434ea-330f-4f3d-e73c-77a901b0c253"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 10 Rarer Words from our example text :- \n","['dihapus', 'yang', 'aritmatika', 'simbol', 'semua', 'dengan', 'matematika', 'ujian', 'jawaban', 'kunci']\n","\n","Result after removing rare words :-\n","Untuk membangun aplikasi Anda , Anda memberi data pelatihan tentang setiap rumah ke dalam algoritma pembelajaran mesin Anda . Ini seperti memiliki untuk .\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["**Implementation of removing single characters**"],"metadata":{"id":"0N-HOEZEGXU9"}},{"cell_type":"code","source":["import re\n","\n","def remove_single_char(text):\n","    \"\"\"\n","    Return :- string after removing single characters\n","    Input  :- string\n","    Output :- string\n","    \"\"\"\n","\n","    single_char_pattern = r'\\b[a-zA-Z]\\b'\n","    without_sc = re.sub(single_char_pattern, '', text)\n","\n","    without_sc = re.sub(r'\\s{2,}', ' ', without_sc).strip()\n","    return without_sc\n","\n","\n","ex_sc = \"ini adalah contoh karakter tunggal seperti a, b, dan c.\"\n","\n","\n","sc_result = remove_single_char(ex_sc)\n","print(f\"Result :-\\n{sc_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uAWJNDa4GwHr","executionInfo":{"status":"ok","timestamp":1756475793311,"user_tz":-420,"elapsed":73,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"d3aa7e26-68ae-487d-8476-245cc41b34e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Result :-\n","ini adalah contoh karakter tunggal seperti , , dan .\n"]}]},{"cell_type":"markdown","source":["**Implementation of removing extra whitespaces**"],"metadata":{"id":"ifRouav8HdJX"}},{"cell_type":"code","source":["import re\n","\n","def remove_extra_spaces(text):\n","    \"\"\"\n","    Return :- string after removing extra whitespaces\n","    Input  :- String\n","    Output :- String\n","    \"\"\"\n","    space_pattern = r'\\s+'\n","    without_space = re.sub(pattern=space_pattern, repl=\" \", string=text).strip()\n","    return without_space\n","\n","\n","ex_space = \"\"\"\n","ini adalah\n","\n","\n","spasi tambahan.\n","\"\"\"\n","\n","\n","space_result = remove_extra_spaces(ex_space)\n","print(f\"Result :- \\n{space_result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vIPgBgjwHxNw","executionInfo":{"status":"ok","timestamp":1756476111892,"user_tz":-420,"elapsed":95,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"814aba13-7db7-44c6-aaf6-3126312575ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Result :- \n","ini adalah spasi tambahan.\n"]}]},{"cell_type":"markdown","source":["**Implementation of Complete preprocessing techniques**"],"metadata":{"id":"CYrjkreWIsXP"}},{"cell_type":"code","source":["import pandas as pd\n","from preprocessing import Preprocess\n","\n","dataset = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, encoding='utf-8')\n","\n","text_column = dataset[1]\n","sentences = text_column.tolist()\n","techniques = [\"lcc\", \"rurls\", \"sc\", \"ntw\", \"res\"]\n","\n","print(\"******** Before preprocessing technique *******\")\n","for sent in sentences[:5]:\n","    print(sent)\n","\n","preprocessing = Preprocess()\n","\n","try:\n","    preprocessed_text = preprocessing.preprocessing(sentences, techniques)\n","    print(\"\\n******** After preprocessing ****************\")\n","    for sent in preprocessed_text[:5]:\n","        print(sent)\n","except Exception as e:\n","    print(f\"\\nError during preprocessing: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399},"id":"i8PzKMx9IzcS","executionInfo":{"status":"error","timestamp":1756549240417,"user_tz":-420,"elapsed":367,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"1f47ea38-7737-4f38-8a3f-acb64f80a901"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'Preprocess' from 'preprocessing' (/usr/local/lib/python3.12/dist-packages/preprocessing/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3028385928.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SMSSpamCollection'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'Preprocess' from 'preprocessing' (/usr/local/lib/python3.12/dist-packages/preprocessing/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"id":"20c4062c","executionInfo":{"status":"ok","timestamp":1756549672036,"user_tz":-420,"elapsed":38,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a430e614-794f-4220-b00c-c3ca3bc2112d"},"source":["import re\n","\n","class Preprocess:\n","    def preprocessing(self, sentences, techniques):\n","        preprocessed_sentences = []\n","        for sentence in sentences:\n","            for tech in techniques:\n","                if tech == \"lcc\":\n","                    sentence = sentence.lower()\n","                elif tech == \"rurls\":\n","                    sentence = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', sentence)\n","                elif tech == \"sc\":\n","                    sentence = sentence.replace(\"teh\", \"the\")\n","                elif tech == \"ntw\":\n","                    sentence = sentence.replace(\"1\", \"satu\").replace(\"2\", \"dua\")\n","                elif tech == \"res\":\n","                    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n","            preprocessed_sentences.append(sentence)\n","        return preprocessed_sentences\n","\n","sentences = [\n","    \"Teh cuaca hari ini sangat cerah dan menyenangkan.\",\n","    \"Kunjungi http://contoh.com untuk informasi lebih lanjut!\",\n","    \"Saya memiliki 2 kucing dan 1 anjing di rumah.\",\n","    \"Liburan ke Bali adalah pengalaman yang luar biasa.\",\n","    \"Harga tiket pesawat naik 2 kali lipat sejak bulan lalu.\"\n","]\n","\n","techniques = [\"lcc\", \"rurls\", \"sc\", \"ntw\", \"res\"]\n","\n","preprocessing = Preprocess()\n","preprocessed_text = preprocessing.preprocessing(sentences, techniques)\n","\n","print(\"\\n******** Sebelum Preprocessing ********\")\n","for sent in sentences:\n","    print(sent)\n","\n","print(\"\\n******** Setelah Preprocessing ********\")\n","for sent in preprocessed_text:\n","    print(sent)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","******** Sebelum Preprocessing ********\n","Teh cuaca hari ini sangat cerah dan menyenangkan.\n","Kunjungi http://contoh.com untuk informasi lebih lanjut!\n","Saya memiliki 2 kucing dan 1 anjing di rumah.\n","Liburan ke Bali adalah pengalaman yang luar biasa.\n","Harga tiket pesawat naik 2 kali lipat sejak bulan lalu.\n","\n","******** Setelah Preprocessing ********\n","the cuaca hari ini sangat cerah dan menyenangkan.\n","kunjungi untuk informasi lebih lanjut!\n","saya memiliki dua kucing dan satu anjing di rumah.\n","liburan ke bali adalah pengalaman yang luar biasa.\n","harga tiket pesawat naik dua kali lipat sejak bulan lalu.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":550},"id":"3dff2869","executionInfo":{"status":"error","timestamp":1756549548576,"user_tz":-420,"elapsed":382,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"cf22f6e9-bb48-4cc7-c27e-8ffb13513523"},"source":["import re\n","import string\n","import nltk\n","from collections import Counter\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","from autocorrect import Speller\n","import unidecode\n","from num2words import num2words\n","import contractions\n","from bs4 import BeautifulSoup\n","\n","# Download necessary NLTK data\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    nltk.download('punkt')\n","try:\n","    nltk.data.find('tokenizers/punkt_tab')\n","except LookupError:\n","    nltk.download('punkt_tab')\n","try:\n","    nltk.data.find('corpora/wordnet')\n","except LookupError:\n","    nltk.download('wordnet')\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except LookupError:\n","    nltk.download('stopwords')\n","\n","\n","# Assuming EMOTICONS list is defined elsewhere in the notebook\n","# Assuming chat_words_map_dict and chat_words_list are defined elsewhere\n","\n","class Preprocess:\n","    def __init__(self):\n","        self.spell_corrector_auto = Speller(lang='en')\n","        self.stemmer = PorterStemmer()\n","        self.lemmatizer = WordNetLemmatizer()\n","        # Define stopwords here or make sure they are globally accessible if needed by a method\n","        self.stopwords_nltk = set(nltk.corpus.stopwords.words('english'))\n","        # Add other necessary initializations if needed\n","\n","    def lower_case_convertion(self, text):\n","        return text.lower()\n","\n","    def remove_html_tags(self, text):\n","        html_pattern = r'<.*?>'\n","        without_html = re.sub(pattern=html_pattern, repl='', string=text)\n","        return without_html\n","\n","    def remove_html_tags_beautifulsoup(self, text):\n","        parser = BeautifulSoup(text, \"html.parser\")\n","        without_html = parser.get_text(separator=\" \")\n","        return without_html\n","\n","    def remove_urls(self, text):\n","        url_pattern = r'https?://\\S+|www\\.\\S+'\n","        without_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n","        return without_urls\n","\n","    def remove_numbers(self, text):\n","        number_pattern = r'\\d+'\n","        without_number = re.sub(pattern=number_pattern, repl=' ', string=text)\n","        return without_number\n","\n","    def num_to_words(self, text):\n","        after_splitting = text.split()\n","        for index in range(len(after_splitting)):\n","            if after_splitting[index].isdigit():\n","                after_splitting[index] = num2words(after_splitting[index])\n","        numbers_to_words = ' '.join(after_splitting)\n","        return numbers_to_words\n","\n","    def spell_autocorrect(self, text):\n","        correct_spell_words = []\n","        for word in word_tokenize(text):\n","            correct_word = self.spell_corrector_auto(word)\n","            correct_spell_words.append(correct_word)\n","        # Need to detokenize correctly, word_tokenize breaks up punctuation\n","        return TreebankWordDetokenizer().detokenize(correct_spell_words)\n","\n","\n","    def accented_to_ascii(self, text):\n","        return unidecode.unidecode(text)\n","\n","    def short_to_original(self, text):\n","        # Assuming chat_words_map_dict and chat_words_list are globally accessible or passed\n","        new_text = []\n","        for w in text.split():\n","            if w.upper() in globals().get('chat_words_list', {}):\n","                 new_text.append(globals().get('chat_words_map_dict', {}).get(w.upper(), w))\n","            else:\n","                new_text.append(w)\n","        return \" \".join(new_text)\n","\n","    def expand_contractions(self, text):\n","        return contractions.fix(text)\n","\n","\n","    def porter_stemmer(self, text):\n","        tokens = word_tokenize(text)\n","        return ' '.join([self.stemmer.stem(token) for token in tokens])\n","\n","    def lemmatization(self, text):\n","        tokens = word_tokenize(text)\n","        return ' '.join([self.lemmatizer.lemmatize(token) for token in tokens])\n","\n","    def remove_emojis(self, text):\n","        emoji_pattern = re.compile(\n","            \"[\"\n","            u\"\\U0001F600-\\U0001F64F\"\n","            u\"\\U0001F300-\\U0001F5FF\"\n","            u\"\\U0001F680-\\U0001F6FF\"\n","            u\"\\U0001F1E0-\\U0001F1FF\"\n","            u\"\\U00002500-\\U00002BEF\"\n","            u\"\\U00002702-\\U000027B0\"\n","            u\"\\U000024C2-\\U0001F251\"\n","            u\"\\U0001f926-\\U0001f937\"\n","            u\"\\U00010000-\\U0010ffff\"\n","            u\"\\u2640-\\u2642\"\n","            u\"\\u2600-\\u2B55\"\n","            u\"\\u200d\"\n","            u\"\\u23cf\"\n","            u\"\\u23e9\"\n","            u\"\\u231a\"\n","            u\"\\ufe0f\"\n","            u\"\\u3030\"\n","            \"]+\", flags=re.UNICODE\n","        )\n","        return emoji_pattern.sub(r'', text)\n","\n","    def remove_emoticons(self, text):\n","        # Assuming EMOTICONS list is globally accessible or passed\n","        emoticon_pattern = re.compile(u'(' + u'|'.join(re.escape(k) for k in globals().get('EMOTICONS', [])) + u')')\n","        without_emoticons = emoticon_pattern.sub(r'', text)\n","        return without_emoticons\n","\n","    def emoji_words(self, text):\n","        # Assuming EMO_UNICODE is globally accessible or passed\n","        if 'EMO_UNICODE' not in globals():\n","            print(\"EMO_UNICODE dictionary is not defined.\")\n","            return text # Return original text or handle error appropriately\n","        for emoji_char, description in globals().get('EMO_UNICODE', {}).items():\n","            emoji_pattern = re.escape(emoji_char)\n","            emoji_name = description\n","            text = re.sub(emoji_pattern, emoji_name, text)\n","        return text\n","\n","    def emoticons_words(self, text):\n","         EMOTICONS_DICT = {\n","            \":-)\": \"smiling_face\",\n","            \":-D\": \"grinning_face\",\n","            \":-(\": \"sad_face\",\n","            \";-)\": \"winking_face\",\n","            # Add more emoticons and their word representations here\n","        }\n","         for emot, word_representation in EMOTICONS_DICT.items():\n","            emoticon_pattern = re.escape(emot)\n","            text = re.sub(emoticon_pattern, word_representation, text)\n","         return text\n","\n","\n","    def remove_punctuation(self, text):\n","        return text.translate(str.maketrans('', '', string.punctuation))\n","\n","    def remove_stopwords(self, text):\n","        text_tokens = word_tokenize(text)\n","        # Use self.stopwords_nltk or combine with others as needed\n","        text_without_sw = [word for word in text_tokens if word.lower() not in self.stopwords_nltk]\n","        return ' '.join(text_without_sw)\n","\n","\n","    def freq_words(self, text, number_frequent_words=10):\n","        tokens = word_tokenize(text.lower())\n","        words = [word for word in tokens if word.isalpha()]\n","        counter = Counter(words)\n","        return [word for word, count in counter.most_common(number_frequent_words)]\n","\n","    def remove_fw(self, text, FrequentWords):\n","        tokens = word_tokenize(text)\n","        filtered = [word for word in tokens if word.lower() not in FrequentWords]\n","        return TreebankWordDetokenizer().detokenize(filtered)\n","\n","\n","    def rare_words(self, text, number_rare_words=10):\n","        tokens = word_tokenize(text.lower())\n","        counter = Counter(tokens)\n","        rare = counter.most_common()[:-number_rare_words-1:-1]\n","        RareWords = [word for word, count in rare]\n","        return RareWords\n","\n","    def remove_rw(self, text, RareWords):\n","        tokens = word_tokenize(text)\n","        without_rw = [word for word in tokens if word.lower() not in RareWords]\n","        return ' '.join(without_rw)\n","\n","    def remove_single_char(self, text):\n","        single_char_pattern = r'\\b[a-zA-Z]\\b'\n","        without_sc = re.sub(single_char_pattern, '', text)\n","        without_sc = re.sub(r'\\s{2,}', ' ', without_sc).strip()\n","        return without_sc\n","\n","\n","    def remove_extra_spaces(self, text):\n","        space_pattern = r'\\s+'\n","        without_space = re.sub(pattern=space_pattern, repl=\" \", string=text).strip()\n","        return without_space\n","\n","\n","    def preprocessing(self, texts, techniques):\n","        processed_texts = []\n","        for text in texts:\n","            cleaned_text = text\n","            if \"lcc\" in techniques:\n","                cleaned_text = self.lower_case_convertion(cleaned_text)\n","            if \"rht\" in techniques:\n","                cleaned_text = self.remove_html_tags(cleaned_text)\n","            if \"rurls\" in techniques:\n","                cleaned_text = self.remove_urls(cleaned_text)\n","            if \"rn\" in techniques:\n","                cleaned_text = self.remove_numbers(cleaned_text)\n","            if \"ntw\" in techniques:\n","                cleaned_text = self.num_to_words(cleaned_text)\n","            if \"asc\" in techniques:\n","                 cleaned_text = self.accented_to_ascii(cleaned_text)\n","            if \"stc\" in techniques:\n","                 cleaned_text = self.short_to_original(cleaned_text)\n","            if \"ec\" in techniques:\n","                 cleaned_text = self.expand_contractions(cleaned_text)\n","            if \"ps\" in techniques:\n","                 cleaned_text = self.porter_stemmer(cleaned_text)\n","            if \"lm\" in techniques:\n","                 cleaned_text = self.lemmatization(cleaned_text)\n","            if \"re\" in techniques:\n","                 cleaned_text = self.remove_emojis(cleaned_text)\n","            if \"rem\" in techniques:\n","                 cleaned_text = self.remove_emoticons(cleaned_text)\n","            if \"etw\" in techniques:\n","                 cleaned_text = self.emoticons_words(cleaned_text)\n","            if \"rp\" in techniques:\n","                 cleaned_text = self.remove_punctuation(cleaned_text)\n","            if \"rsw\" in techniques:\n","                 cleaned_text = self.remove_stopwords(cleaned_text)\n","            if \"rsc\" in techniques:\n","                 cleaned_text = self.remove_single_char(cleaned_text)\n","            if \"res\" in techniques:\n","                 cleaned_text = self.remove_extra_spaces(cleaned_text)\n","            # Note: Removing frequent/rare words might need more context or be applied differently\n","            # as they depend on the entire corpus, not individual sentences.\n","            # If you need to apply these, you might need to calculate frequent/rare words\n","            # on the whole 'texts' list before iterating through individual sentences.\n","\n","            processed_texts.append(cleaned_text)\n","        return processed_texts"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'acyclic_depth_first' from 'nltk.util' (/usr/local/lib/python3.12/dist-packages/nltk/util.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-653126824.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/translate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbleu_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentence_bleu\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mribes_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentence_ribes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mribes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeteor_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmeteor_score\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmeteor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malignment_error_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_decoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStackDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/translate/meteor_score.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetCorpusReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStemmerI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/corpus/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyCorpusLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbnc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnps_chat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitchboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdependency\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_WordNetObject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m     \"\"\"Create a Synset from a \"<lemma>.<pos>.<number>\" string where:\n\u001b[1;32m    353\u001b[0m     \u001b[0;34m<\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mword\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mmorphological\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mSynset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0macyclic_depth_first\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macyclic_tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munweighted_minimum_spanning_tree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'acyclic_depth_first' from 'nltk.util' (/usr/local/lib/python3.12/dist-packages/nltk/util.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eeeac562","executionInfo":{"status":"ok","timestamp":1756549525676,"user_tz":-420,"elapsed":6437,"user":{"displayName":"Kayla Nuansa Ceria","userId":"12942333872628805677"}},"outputId":"2a224e5f-e490-43b5-c311-a88fc67bd975"},"source":["%pip uninstall -y nltk\n","%pip install nltk==3.8.1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: nltk 3.2.4\n","Uninstalling nltk-3.2.4:\n","  Successfully uninstalled nltk-3.2.4\n","Collecting nltk==3.8.1\n","  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (4.67.1)\n","Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nltk\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\n","textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nltk-3.8.1\n"]}]}]}